# -*- coding: utf-8 -*-
"""whylogs and WhyLabs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g12BAY7IOtnQuAdlVrz5QsmY4JlcBqax
"""

!python --version

# Install whylogs
!pip install -q 'whylogs[viz]'

# Importing packages
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
pd.set_option("display.max_columns", None) # Set to show all columns in dataframe
import whylogs as why

# Commented out IPython magic to ensure Python compatibility.
# %pip show whylogs

import pandas as pd
df = pd.read_csv("https://gist.githubusercontent.com/suhas-ds/a318d2b1dda8d8cbf2d6990a8f0b7e8a/raw/9b548fab0952dd12b8fdf057188038c8950428f1/loan_dataset.csv")

df.head()

df.dtypes

df.ApplicantIncome.nunique()

df.Credit_History.value_counts()

prifile1 = why.log(df)
profile_view1 = prifile1.view()
profile_view1.to_pandas()

#_________________________________Constraints________________________________________-#

from whylogs.core.constraints import (Constraints,
                                     ConstraintsBuilder,
                                     MetricsSelector,
                                     MetricConstraint)

# Function with Constraints for Data Quality Validation

def validate_feat(profile_view, verbose=False, viz=False):

  builder = ConstraintsBuilder(profile_view)
  
  # Defining a constraint for data validation
  builder.add_constraint(MetricConstraint(
    name="Credit_History == 0 or == 1",
    condition=lambda x: x.min == 0 or x.max == 1,
    metric_selector=MetricsSelector(metric_name='distribution',
                                    column_name='Credit_History')
  ))
  
  # Build the constraints
  constraints: Constraints = builder.build()
  
  if verbose:
    print(constraints.report())

  # return constraints.report()
  return constraints

const = validate_feat(profile_view1, True)

# `[('Name', Pass, Fail)]`

from whylogs.viz import NotebookProfileVisualizer
visualization = NotebookProfileVisualizer()
visualization.constraints_report(const, cell_height=300)

# check all constraints for passing:
constraints_valid = const.validate()
print(constraints_valid)

#______________________________ Model Building ______________________________

# Importing the required packages
import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression

# from matplotlib import pyplot as plt
from sklearn import preprocessing
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn import metrics
import os
import datetime as dt

# Reading the data
data = pd.read_csv("https://gist.githubusercontent.com/suhas-ds/a318d2b1dda8d8cbf2d6990a8f0b7e8a/raw/9b548fab0952dd12b8fdf057188038c8950428f1/loan_dataset.csv")
num_col = data.select_dtypes(include=['int64','float64']).columns.tolist()
cat_col = data.select_dtypes(include=['object']).columns.tolist()
cat_col.remove('Loan_Status')
cat_col.remove('Loan_ID')

# Creating list of categorical and numerical variables
for col in cat_col:
    data[col].fillna(data[col].mode()[0], inplace=True)

for col in num_col:
    data[col].fillna(data[col].median(), inplace=True)

# Clipping extreme values
data[num_col] = data[num_col].apply(lambda x: x.clip(*x.quantile([0.05, 0.95])))

# creating new feature as Total Income
data['LoanAmount'] = np.log(data['LoanAmount']).copy()
data['TotalIncome'] = data['ApplicantIncome'] + data['CoapplicantIncome']
data['TotalIncome'] = np.log(data['TotalIncome']).copy()

# Dropping ApplicantIncome and CoapplicantIncome
data = data.drop(['ApplicantIncome','CoapplicantIncome'], axis=1)

# Label encoding categorical variables
for col in cat_col:
    le = preprocessing.LabelEncoder()
    data[col] = le.fit_transform(data[col])

data['Loan_Status'] = le.fit_transform(data['Loan_Status'])

# Train test split
X = data.drop(['Loan_Status', 'Loan_ID'],1)
y = data['Loan_Status']

SEED = 1

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size =0.3, random_state = SEED)
#______________Logistic Regresssion__________________________#
lr = LogisticRegression(random_state=SEED)
lr_param_grid = {
    'C': [100, 10, 1.0, 0.1, 0.01],
    'penalty': ['l1','l2'],
    'solver':['liblinear']
    } 

lr_gs = GridSearchCV(
        estimator=lr,
        param_grid=lr_param_grid, 
        cv=5,
        n_jobs=-1,
        scoring='accuracy',
        verbose=0
        )
lr_model = lr_gs.fit(X, y)

def model_metrics(actual, pred):
    accuracy = metrics.accuracy_score(actual, pred)
    f1 = metrics.f1_score(actual, pred, pos_label=1)
    fpr, tpr, thresholds1 = metrics.roc_curve(actual, pred)
    auc = metrics.auc(fpr, tpr)
    print('accuracy:',accuracy, '\nf1:', f1, '\nauc:', auc)
    return accuracy, f1, auc

#Make Prediction
y_pred = lr_model.predict(X)
# Predicting probability
y_pred_prob = lr_model.predict_proba(X)

# Getting a maximum probability value of class 
y_pred_prob_max = [max(p) for p in y_pred_prob]
y_pred_prob_max

# Generate performance metrics
model_metrics(y,pd.Series(y_pred))

# Output/Prediction column name should contain the 'output' 
data.rename(columns={'Loan_Status':'output_loan_status'}, inplace=True)
data['output_prediction'] = y_pred
data['output_score'] = y_pred_prob_max
data

# Spliting the final dataframe into four 
df1, df2, df3, df4 = np.array_split(data, 4)

#______________________________ Drift detection and comparing two datasets ______________________________

profile_view1 = why.log(df1).view()
profile_view2 = why.log(df2).view()

from whylogs.viz import NotebookProfileVisualizer

visualization = NotebookProfileVisualizer()
visualization.set_profiles(target_profile_view=profile_view1, reference_profile_view=profile_view2)

visualization.profile_summary()

visualization.summary_drift_report()

#_______________________ Single dataset __________________________

whylabs_org = 'org-Exxxx'
whylabs_key = '0JixxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxmD0xxxrL'
whylabs_project = 'model-4'

!pip install -q "whylogs[whylabs]"

import whylogs as why
import pandas as pd
import os
from whylogs.api.writer.whylabs import WhyLabsWriter
import datetime as dt
writer = WhyLabsWriter()
os.environ["WHYLABS_DEFAULT_ORG_ID"] = whylabs_org # ORG-ID is case sensistive
os.environ["WHYLABS_API_KEY"] = whylabs_key
os.environ["WHYLABS_DEFAULT_DATASET_ID"] = whylabs_project

# Importing the required packages
import pandas as pd
import numpy as np

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier

from matplotlib import pyplot as plt

from sklearn import preprocessing
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn import metrics

# Reading the data
data = pd.read_csv("https://gist.githubusercontent.com/suhas-ds/a318d2b1dda8d8cbf2d6990a8f0b7e8a/raw/9b548fab0952dd12b8fdf057188038c8950428f1/loan_dataset.csv")
num_col = data.select_dtypes(include=['int64','float64']).columns.tolist()
cat_col = data.select_dtypes(include=['object']).columns.tolist()
cat_col.remove('Loan_Status')
cat_col.remove('Loan_ID')

# Creating list of categorical and numerical variables
for col in cat_col:
    data[col].fillna(data[col].mode()[0], inplace=True)

for col in num_col:
    data[col].fillna(data[col].median(), inplace=True)

# Clipping extreme values
data[num_col] = data[num_col].apply(lambda x: x.clip(*x.quantile([0.05, 0.95])))

# creating new feature as Total Income
data['LoanAmount'] = np.log(data['LoanAmount']).copy()
data['TotalIncome'] = data['ApplicantIncome'] + data['CoapplicantIncome']
data['TotalIncome'] = np.log(data['TotalIncome']).copy()

# Dropping ApplicantIncome and CoapplicantIncome
data = data.drop(['ApplicantIncome','CoapplicantIncome'], axis=1)

# Label encoding categorical variables
for col in cat_col:
    le = preprocessing.LabelEncoder()
    data[col] = le.fit_transform(data[col])

data['Loan_Status'] = le.fit_transform(data['Loan_Status'])

# Train test split
X = data.drop(['Loan_Status', 'Loan_ID'],1)
y = data['Loan_Status']

SEED = 1

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size =0.3, random_state = SEED)
#______________Logistic Regresssion__________________________#
lr = LogisticRegression(random_state=SEED)
lr_param_grid = {
    'C': [100, 10, 1.0, 0.1, 0.01],
    'penalty': ['l1','l2'],
    'solver':['liblinear']
}

lr_gs = GridSearchCV(
        estimator=lr,
        param_grid=lr_param_grid, 
        cv=5,
        n_jobs=-1,
        scoring='accuracy',
        verbose=0
    )
lr_model = lr_gs.fit(X, y)

def model_metrics(actual, pred):
    accuracy = metrics.accuracy_score(y_test, pred)
    f1 = metrics.f1_score(actual, pred, pos_label=1)
    fpr, tpr, thresholds1 = metrics.roc_curve(y_test, pred)
    auc = metrics.auc(fpr, tpr)
    print('accuracy:',accuracy, 'f1:', f1, 'auc:', auc)
    return accuracy, f1, auc

y_pred = lr_model.predict(X)
y_pred_prob = lr_model.predict_proba(X)

# Generate performance metrics
# model_metrics(y,pd.Series(pred))
metrics.accuracy_score(y,pd.Series(y_pred))

y_pred_prob_max = [max(p) for p in y_pred_prob]

date = dt.datetime.now(tz=dt.timezone.utc) - dt.timedelta(days=1)

y_pred_df = pd.DataFrame(y_pred, columns=['output'])

writer = WhyLabsWriter()
profile = why.log(X).profile()

date = dt.datetime.now(tz=dt.timezone.utc) - dt.timedelta(days=4)
#set the dataset timestamp for the profile
profile.set_dataset_timestamp(date)
#write the profile to the WhyLabs platform
writer.write(file=profile.view())

writer = WhyLabsWriter()

profile = why.log(y_pred_df).profile()

date = dt.datetime.now(tz=dt.timezone.utc) - dt.timedelta(days=4)
# set the dataset timestamp for the profile
profile.set_dataset_timestamp(date)
#write the profile to the WhyLabs platform
writer.write(file=profile.view())

# pd.concat([data, y_pred_df], axis=1)
data.rename(columns={'Loan_Status':'output_loan_status'}, inplace=True)
data['output_prediction'] = y_pred
data['output_score'] = y_pred_prob_max
data

date = dt.datetime.now(tz=dt.timezone.utc) - dt.timedelta(days=4)
print("logging data for date {}".format(date))
results = why.log_classification_metrics(
    data,
    target_column = "output_loan_status",
    prediction_column = "output_prediction",
    score_column="output_score"
)

profile = results.profile()
profile.set_dataset_timestamp(date)

print("writing profiles to whylabs...")
results.writer("whylabs").write()

#_____________________Batches_________________________

# WhyLabs 
whylabs_org = 'org-Exxxxx'
whylabs_key = '0Ji9w2gC7q.lQQ4xxxxxxxxxxxxxxxxxxxxxxxxxxxxMotDmxxxxxxDrL'
whylabs_project = 'model-5'

# Importing the required packages
import whylogs as why
from whylogs.api.writer.whylabs import WhyLabsWriter
import os
import datetime as dt
writer = WhyLabsWriter()
os.environ["WHYLABS_DEFAULT_ORG_ID"] = whylabs_org # ORG-ID is case sensistive
os.environ["WHYLABS_API_KEY"] = whylabs_key
os.environ["WHYLABS_DEFAULT_DATASET_ID"] = whylabs_project

daily_batches = [df1, df2, df3, df4]

df1.head()

df['output_prediction']

for i, data_frame in enumerate(daily_batches):
    date_time = dt.datetime.now(tz=dt.timezone.utc) - dt.timedelta(days=i)

    df = data_frame
    print("logging data for date {}".format(date_time))
    results = why.log_classification_metrics(
        df,
        target_column = "output_loan_status",
        prediction_column = "output_prediction",
        score_column="output_score"
    )

    profile = results.profile()
    profile.set_dataset_timestamp(date_time)

    print("writing profiles to whylabs...")
    results.writer("whylabs").write()

